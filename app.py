import io

from io import BytesIO

import re

import numpy as np

import pandas as pd

import plotly.express as px

import streamlit as st

from unidecode import unidecode

import base64

# -------------------- CONFIG G√âN√âRALE --------------------

st.set_page_config(page_title="Qualit√© & Uniformisation ‚Äì Tableaux Monday", layout="wide")

# Palette de couleurs Accor Premium

ACCOR_COLS = ["#1d3557", "#457b9d", "#a8dadc", "#f1faee", "#bde0fe", "#cdb4db"]


# Fonction : lire le logo Accor et le convertir en base64

def get_base64_image(image_path: str) -> str:

    with open(image_path, "rb") as img_file:

        return base64.b64encode(img_file.read()).decode()


image_base64 = get_base64_image("logo_accor.png")

# --- HEADER / BANNI√àRE ACCOR √©quilibr√©e ---

st.markdown(

    f"""
<style>

  /* Retirer la marge de Streamlit autour du contenu */

  .block-container {{

      padding-top: 5%;

      padding-bottom: 50%;

      max-width: 100%;

  }}

  /* Banni√®re dor√©e pleine largeur */

  .fullwidth-header {{

      width: 100%;

      background-color: #fef6e4;

      display: flex;

      align-items: center;

      justify-content: space-between;

      padding: 18px 70px;

      box-sizing: border-box;

      box-shadow: 0 4px 8px rgba(0,0,0,0.05);

      border-bottom: 1px solid #e5d9b6;

  }}

  /* Logo Accor */

  .fullwidth-header img {{

      height: 110px;

  }}

  /* Titre */

  .fullwidth-header .title {{

      flex-grow: 1;

      text-align: center;

      font-size: 40px;

      font-weight: 700;

      color: #1d3557;

      letter-spacing: 0.5px;

  }}

  /* Sous-titre */

  .subtext {{

      text-align: center;

      color: #666;

      padding-left: 20%;

      font-size: 15px;

      margin-top: 10px;

      margin-bottom: 55px;

  }}

  /* KPI cards */

  .kpi-container {{

      display: flex;

      justify-content: space-around;

      margin-top: 25px;

      margin-bottom: 35px;

      gap: 10px;

  }}

  .kpi-card {{

      background-color: white;

      border: 2px solid #e5d9b6;

      border-radius: 12px;

      width: 23%;

      padding: 20px;

      text-align: center;

      box-shadow: 0 4px 8px rgba(0,0,0,0.1);

      transition: transform 0.2s ease;

  }}

  .kpi-card:hover {{

      transform: translateY(-4px);

  }}

  .kpi-title {{

      color: #1d3557;

      font-weight: 600;

      font-size: 18px;

      margin-bottom: 10px;

  }}

  .kpi-value {{

      font-size: 32px;

      font-weight: 800;

      color: #1d3557;

  }}

  .kpi-subtext {{

      font-size: 12px;

      color: #666;

      margin-top: 4px;

  }}
</style>
<!-- Banni√®re -->
<div class="fullwidth-header">
<img src="data:image/png;base64,{image_base64}" alt="Accor Logo">
<div class="title"> Qualit√© & Uniformisation ‚Äì Tableaux Monday</div>
</div>
<!-- Sous-titre -->
<p class="subtext">

    Analyse de coh√©rence et de compl√©tude des donn√©es issues de Monday
</p>

""",

    unsafe_allow_html=True,

)

# -------------------- PARAMS --------------------

# Colonnes √† analyser (tu peux adapter l'intitul√© EXACT de Monday ici)

COLONNES_CIBLES = [

    "Cat√©gorie",

    "Cl√¥ture / Qualit√©",

    "Outil",

    "Outils secondaires",  # l'app acceptera aussi "Outils secondaire"

    "P√¥le",

    "Cycle comptable",

    "Temps moyen th√©orique en h (1=1h, 0,25 = 15min)",

    "R√©currence",

    "Mois de cl√¥ture",

    "Statut",

    "Soci√©t√©s juridiques",

    "Niveau de p√©nibilit√© (ressenti)",

    "Notifier comptable",

    "Statut de la proc√©dure",

    "Statut du mode op√©ratoire",

]

# Variantes tol√©r√©es de noms de colonnes -> nom canonique

CANON_MAP = {

    "outils secondaire": "Outils secondaires",

    "outils secondaires": "Outils secondaires",

    "pole": "P√¥le",

    "p√¥le": "P√¥le",

    "societes juridiques": "Soci√©t√©s juridiques",

    "mois de cloture": "Mois de cl√¥ture",

    "temps moyen theorique en h (1=1h, 0,25 = 15min)": "Temps moyen th√©orique en h (1=1h, 0,25 = 15min)",

    "cloture / qualite": "Cl√¥ture / Qualit√©",

    "statut du mode operatoire": "Statut du mode op√©ratoire",

    "statut de la procedure": "Statut de la proc√©dure",

    "niveau de penibilite (ressenti)": "Niveau de p√©nibilit√© (ressenti)",

}

# -------------------- HELPERS --------------------

def normalise_col(c: str) -> str:

    """Normalise un nom de colonne (pour matcher les variantes)."""

    key = unidecode(str(c)).lower().strip()

    if key in CANON_MAP:

        return CANON_MAP[key]

    for cible in COLONNES_CIBLES:

        if unidecode(cible).lower().strip() == key:

            return cible

    return c


def read_any_table(uploaded_file: BytesIO) -> pd.DataFrame:

    name = uploaded_file.name.lower()

    if name.endswith(".ods"):

        return pd.read_excel(uploaded_file, engine="odf")

    else:

        return pd.read_excel(uploaded_file)


def completeness_table(df: pd.DataFrame, colonne: str, by_pole: bool = True) -> pd.DataFrame:

    """Calcule le taux de remplissage global et par p√¥le en nettoyant les fausses valeurs vides."""

    if colonne not in df.columns:

        return pd.DataFrame(columns=["Colonne", "P√¥le", "Remplies", "Vides", "Taux (%)"])

    s = (

        df[colonne]

        .astype(str)

        .replace(["nan", "NaN", "None"], "")

        .str.replace(r"[\r\n\t]", "", regex=True)

        .str.strip()

    )

    total = len(s)

    non_vides = s.apply(lambda x: bool(x)).sum()

    taux = round(non_vides / total * 100, 2) if total > 0 else 0

    out = [

        {

            "Colonne": colonne,

            "P√¥le": "Global",

            "Remplies": non_vides,

            "Vides": total - non_vides,

            "Taux (%)": taux,

        }

    ]

    if by_pole and "P√¥le" in df.columns:

        for pole, sub in df.groupby("P√¥le", dropna=False):

            s_p = (

                sub[colonne]

                .astype(str)

                .replace(["nan", "NaN", "None"], "")

                .str.replace(r"[\r\n\t]", "", regex=True)

                .str.strip()

            )

            total_p = len(s_p)

            non_vides_p = s_p.apply(lambda x: bool(x)).sum()

            taux_p = round(non_vides_p / total_p * 100, 2) if total_p > 0 else 0

            out.append(

                {

                    "Colonne": colonne,

                    "P√¥le": pole,

                    "Remplies": non_vides_p,

                    "Vides": total_p - non_vides_p,

                    "Taux (%)": taux_p,

                }

            )

    return pd.DataFrame(out)


def is_multivalue_column(series: pd.Series) -> bool:

    """D√©tecte si une colonne contient souvent plusieurs valeurs dans une m√™me cellule."""

    sample = series.dropna().astype(str)

    multi_ratio = sample.str.contains(r"[,\n;]", regex=True).mean()

    return multi_ratio > 0.05  # seuil 5% de cellules multi-valu√©es


def unique_values_table(df: pd.DataFrame, colonne: str) -> pd.DataFrame:

    """Retourne les valeurs uniques d'une colonne."""

    if colonne not in df.columns:

        return pd.DataFrame(columns=["Colonne", "Valeur unique"])

    series = df[colonne].fillna("").astype(str)

    vals = []

    if is_multivalue_column(series):

        for cell in series:

            parts = re.split(r"[,\n;]", str(cell))

            for p in parts:

                p = p.strip()

                if p:

                    vals.append(p)

    else:

        vals = [v.strip() for v in series if v.strip() not in ["", "nan", "None"]]

    uniques = sorted(set(vals))

    return pd.DataFrame({"Colonne": colonne, "Valeur unique": uniques})


def normalize_value_for_compare(v: str) -> str:

    """Normalise un texte pour rep√©rer les incoh√©rences d'√©criture."""

    v = str(v)

    v = v.replace("\xa0", " ")

    v = v.replace("\u200b", "")

    v = re.sub(r"[\r\n\t]", " ", v)

    v = unidecode(v).lower().strip()

    v = re.sub(r"[\s\-_]+", " ", v)

    v = re.sub(r"[^\w\s&/]", "", v)

    return v


def incoherence_table(df: pd.DataFrame, colonne: str):

    """

    Analyse les incoh√©rences d‚Äôune colonne :

    - normalise les √©critures

    - d√©tecte les variantes

    - identifie les fichiers o√π chaque variante appara√Æt

    """

    if colonne not in df.columns:

        return (

            pd.DataFrame(

                columns=[

                    "Colonne",

                    "Cl√© normalis√©e",

                    "Variantes",

                    "Nb variantes",

                    "Total occ.",

                    "Fichiers concern√©s",

                ]

            ),

            pd.DataFrame(

                columns=[

                    "Colonne",

                    "Valeur unique propre",

                    "Fichiers concern√©s",

                ]

            ),

        )

    # On r√©cup√®re valeur + fichier source

    if "__source_file__" in df.columns:

        series = df[[colonne, "__source_file__"]].fillna("")

        use_source = True

    elif "Source fichier" in df.columns:

        series = df[[colonne, "Source fichier"]].fillna("")

        series = series.rename(columns={"Source fichier": "__source_file__"})

        use_source = True

    else:

        series = df[[colonne]].fillna("")

        series["__source_file__"] = ""

        use_source = False

    vals = []

    for _, row in series.iterrows():

        cell = str(row[colonne])

        fichier = row["__source_file__"]

        parts = re.split(r"[,\n;]", cell)

        for p in parts:

            p = p.strip()

            if p:

                vals.append((p, fichier))

    if not vals:

        return (

            pd.DataFrame(

                columns=[

                    "Colonne",

                    "Cl√© normalis√©e",

                    "Variantes",

                    "Nb variantes",

                    "Total occ.",

                    "Fichiers concern√©s",

                ]

            ),

            pd.DataFrame(

                columns=[

                    "Colonne",

                    "Valeur unique propre",

                    "Fichiers concern√©s",

                ]

            ),

        )

    tmp = pd.DataFrame(vals, columns=["val", "file"])

    tmp["key"] = tmp["val"].apply(normalize_value_for_compare)

    incoherences = []

    propres = []

    for key, g in tmp.groupby("key"):

        variantes = sorted(g["val"].unique())

        fichiers = sorted([f for f in g["file"].unique() if f != ""])

        total = len(g)

        if len(variantes) > 1:

            incoherences.append(

                {

                    "Colonne": colonne,

                    "Cl√© normalis√©e": key,

                    "Variantes": ", ".join(variantes),

                    "Nb variantes": len(variantes),

                    "Total occ.": total,

                    "Fichiers concern√©s": ", ".join(fichiers) if use_source else "",

                }

            )

        else:

            propres.append(

                {

                    "Colonne": colonne,

                    "Valeur unique propre": variantes[0],

                    "Fichiers concern√©s": ", ".join(fichiers) if use_source else "",

                }

            )

    incoh_df = pd.DataFrame(incoherences)

    propres_df = pd.DataFrame(propres)

    return incoh_df, propres_df


def to_excel_bytes(dfs: dict) -> bytes:

    buffer = io.BytesIO()

    with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:

        for sheet, frame in dfs.items():

            frame.to_excel(writer, sheet_name=sheet, index=False)

    buffer.seek(0)

    return buffer.getvalue()


# -------------------- UI : UPLOAD & OPTIONS --------------------

st.caption(

    "Charge un ou plusieurs exports Excel **.xlsx** ou **.ods** fusionn√©s depuis Monday. "

    "L‚Äôapp calcule : KPIs, compl√©tude globale & par p√¥le, valeurs distinctes, "

    "et **incoh√©rences d‚Äô√©criture** √† corriger dans Monday."

)

uploaded_files = st.file_uploader(

    "Fichiers d'entr√©e (.xlsx ou .ods) ‚Äì vous pouvez en charger plusieurs",

    type=["xlsx", "ods"],

    accept_multiple_files=True,

)

if not uploaded_files:

    st.info("üëâ Importez au moins un fichier pour lancer l‚Äôanalyse.")

    st.stop()

dfs_import = []

for f in uploaded_files:

    df_tmp = read_any_table(f)

    df_tmp["Source fichier"] = f.name

    dfs_import.append(df_tmp)

df = pd.concat(dfs_import, ignore_index=True)

df["__source_file__"] = df["Source fichier"]

st.success(f"‚úÖ {len(uploaded_files)} fichier(s) import√©(s) ‚Äì {len(df):,} lignes fusionn√©es")

# Options : colonnes analytiques √† utiliser (heatmap + taux de remplissage analyse)

with st.expander("‚öôÔ∏è Options d‚Äôanalyse", expanded=False):

    cols_selection = st.multiselect(

        "Colonnes √† analyser (pour la heatmap & le taux de remplissage d‚Äôanalyse)",

        options=COLONNES_CIBLES,

        default=COLONNES_CIBLES,

    )

# -------------------- MODE D'ANALYSE --------------------

st.write("")

st.write("### ‚öôÔ∏è Choisir le mode d‚Äôanalyse")

mode_unique = st.radio(

    "Comment analyser les donn√©es ?",

    ["Par ligne (fichier brut)", "Par t√¢che unique (colonne Name)"],

    index=0,

    horizontal=True,

)

# Nettoyage P√¥le

if "P√¥le" in df.columns:

    df["P√¥le"] = (

        df["P√¥le"]

        .astype(str)

        .str.strip()

        .str.upper()

        .replace({"NAN": ""})

    )

# Mode 'Par t√¢che unique'

if mode_unique == "Par t√¢che unique (colonne Name)" and "Name" in df.columns:

    lignes_avant = len(df)

    df = df.drop_duplicates(keep="first")

    lignes_apres = len(df)

    nb_doublons = lignes_avant - lignes_apres

    st.info(

        f"üßæ {nb_doublons} doublon(s) strict(s) supprim√©(s). "

        f"{lignes_apres:,} lignes uniques restantes."

    )

else:

    st.caption(

        "Chaque ligne des fichiers est consid√©r√©e comme un enregistrement brut "

        "(aucune suppression de doublons)."

    )

# Normaliser les noms de colonnes

df.columns = [normalise_col(c) for c in df.columns]

# Feedback colonnes manquantes

missing = [c for c in cols_selection if c not in df.columns]

if missing:

    st.warning("Colonnes non trouv√©es dans les fichiers : " + ", ".join(missing))

# Retenir seulement colonnes pr√©sentes

cols_presentes = [c for c in cols_selection if c in df.columns]

if mode_unique == "Par t√¢che unique (colonne Name)":

    st.write("**Analyse effectu√©e sur les t√¢ches uniques** (doublons supprim√©s).")

else:

    st.write("**Analyse effectu√©e sur les lignes brutes des fichiers fusionn√©s.**")

# -------------------- COMPLETENESS + KPI COH√âRENT --------------------

taux_frames = []

for c in cols_presentes:

    taux_frames.append(completeness_table(df, c, by_pole=True))

taux = (

    pd.concat(taux_frames, ignore_index=True)

    if taux_frames

    else pd.DataFrame(columns=["Colonne", "P√¥le", "Remplies", "Vides", "Taux (%)"])

)

pivot = None

kpi_coherent = 0.0

if not taux.empty and "P√¥le" in taux.columns:

    pivot = taux.pivot_table(

        index="Colonne",

        columns="P√¥le",

        values="Taux (%)",

        aggfunc="mean",

    )

    valeurs_heatmap = pivot.values.flatten()

    valeurs_heatmap = [v for v in valeurs_heatmap if not np.isnan(v)]

    if valeurs_heatmap:

        kpi_coherent = round(np.mean(valeurs_heatmap), 2)

    else:

        kpi_coherent = 0.0

else:

    kpi_coherent = 0.0

# -------------------- KPIs --------------------

nb_fichiers = len(uploaded_files)

nb_taches = len(df)

nb_poles = df["P√¥le"].nunique(dropna=True) if "P√¥le" in df.columns else 0

taux_moy = kpi_coherent  # KPI coh√©rent avec la heatmap

st.markdown(

    f"""
<div class="kpi-container">
<div class="kpi-card">
<div class="kpi-title">Fichiers import√©s</div>
<div class="kpi-value">{nb_fichiers}</div>
<div class="kpi-subtext">Exports Monday fusionn√©s</div>
</div>
<div class="kpi-card">
<div class="kpi-title">Nombre total de t√¢ches</div>
<div class="kpi-value">{nb_taches:,}</div>
<div class="kpi-subtext">Lignes apr√®s concat√©nation</div>
</div>
<div class="kpi-card">
<div class="kpi-title">P√¥les distincts</div>
<div class="kpi-value">{nb_poles}</div>
<div class="kpi-subtext">Valeurs uniques de la colonne P√¥le</div>
</div>
<div class="kpi-card">
<div class="kpi-title">Taux de remplissage (analyse)</div>
<div class="kpi-value">{taux_moy:.2f}%</div>
<div class="kpi-subtext">Bas√© uniquement sur les colonnes analytiques (heatmap)</div>
</div>
</div>

""",

    unsafe_allow_html=True,

)

st.write("")

st.write("")

# -------------------- VISU 1 : T√ÇCHES PAR P√îLE --------------------

if "P√¥le" in df.columns:

    taches_pole = (

        df["P√¥le"]

        .fillna("NON RENSEIGN√â")

        .value_counts()

        .reset_index()

    )

    taches_pole.columns = ["P√¥le", "Nombre_de_taches"]

    fig = px.bar(

        taches_pole,

        x="P√¥le",

        y="Nombre_de_taches",

        title="Nombre de t√¢ches par p√¥le",

        text="Nombre_de_taches",

        color="P√¥le",

        color_discrete_sequence=ACCOR_COLS,

    )

    fig.update_traces(

        textfont_size=14,

        textposition="outside",

        marker_line_color="#fef6e4",

        marker_line_width=1.5,

    )

    fig.update_layout(

        plot_bgcolor="white",

        paper_bgcolor="white",

        title_font=dict(size=20, color="#1d3557", family="Segoe UI"),

        font=dict(size=14, color="#1a1a1a"),

        margin=dict(l=20, r=20, t=60, b=20),

        showlegend=False,

    )

    st.plotly_chart(fig, use_container_width=True)

st.divider()

st.write("")

st.write("")

# -------------------- COMPLETENESS (AFFICHAGE) --------------------

st.subheader("Taux de remplissage (global & par p√¥le)")

st.write("")

if pivot is not None:

    fig2 = px.imshow(

        pivot,

        text_auto=True,

        aspect="auto",

        color_continuous_scale="Blues",

        title="Heatmap ‚Äì Taux de remplissage par colonne et par p√¥le",

    )

    fig2.update_layout(height=520, margin=dict(l=10, r=10, t=50, b=10))

    st.plotly_chart(fig2, use_container_width=True)

st.dataframe(

    taux.sort_values(["Colonne", "P√¥le"]),

    use_container_width=True,

    height=420,

)

st.divider()

st.write("")

st.write("")

# -------------------- TOP 5 COLONNES LES MOINS REMPLIES --------------------

st.subheader("Top 5 des colonnes les moins remplies (global)")

st.write("")

try:

    taux_global = taux[taux["P√¥le"] == "Global"].sort_values(

        "Taux (%)", ascending=True

    )

    top5 = taux_global.head(5)

    fig_top5 = px.bar(

        top5,

        y="Colonne",

        x="Taux (%)",

        orientation="h",

        text="Taux (%)",

        color="Taux (%)",

        color_continuous_scale=[

            "#fef6e4",  # Dor√© clair

            "#a8dadc",  # Bleu pastel

            "#457b9d",  # Bleu doux

            "#1d3557",  # Bleu marine Accor

        ],

        title="Colonnes",

    )

    fig_top5.update_traces(

        texttemplate="%{x:.2f} %",

        textposition="outside",

        marker_line_color="#fef6e4",

        marker_line_width=1.5,

    )

    fig_top5.update_layout(

        paper_bgcolor="white",

        plot_bgcolor="white",

        title_font=dict(size=20, color="#1d3557", family="Segoe UI"),

        font=dict(size=13, color="#1a1a1a"),

        xaxis_title="Taux de remplissage (%)",

        yaxis_title=None,

        height=450,

        margin=dict(l=30, r=30, t=70, b=30),

        # PAS de yaxis reversed : la barre tout en haut = colonne la moins remplie

        coloraxis_colorbar=dict(

            title=dict(

                text="Taux (%)",

                font=dict(color="#1d3557", size=12, family="Segoe UI"),

            ),

            tickfont=dict(color="#1d3557", size=11),

        ),

    )

    st.plotly_chart(fig_top5, use_container_width=True)

except Exception as e:

    st.error(f"Erreur lors de la g√©n√©ration du graphique : {e}")

# -------------------- VALEURS UNIQUES & INCOHERENCES --------------------

st.write("")

st.write("")

st.subheader("Valeurs distinctes & incoh√©rences d‚Äô√©criture")

st.write("")

incoh_list = []

propres_list = []

uniques_list = []

for c in cols_presentes:

    incoh_c, clean_c = incoherence_table(df, c)

    if not incoh_c.empty:

        incoh_list.append(incoh_c)

    if not clean_c.empty:

        propres_list.append(clean_c)

    uniques_list.append(unique_values_table(df, c))

incoherences = (

    pd.concat(incoh_list, ignore_index=True)

    if incoh_list

    else pd.DataFrame(

        columns=[

            "Colonne",

            "Cl√© normalis√©e",

            "Variantes",

            "Nb variantes",

            "Total occ.",

            "Fichiers concern√©s",

        ]

    )

)

propres_final = (

    pd.concat(propres_list, ignore_index=True)

    if propres_list

    else pd.DataFrame(

        columns=[

            "Colonne",

            "Valeur unique propre",

            "Fichiers concern√©s",

        ]

    )

)

uniques = (

    pd.concat(uniques_list, ignore_index=True)

    if uniques_list

    else pd.DataFrame(columns=["Colonne", "Valeur unique"])

)

tab1, tab2, tab3 = st.tabs(

    [

        "Valeurs propres (sans variantes)",

        "Incoh√©rences (√©critures diff√©rentes)",

        "Valeurs brutes (toutes les √©critures)",

    ]

)

# --- Valeurs propres ---

with tab1:

    st.dataframe(propres_final, use_container_width=True, height=450)

    st.caption(

        "Ces valeurs apparaissent sous une seule forme dans les fichiers "

        "(aucune variante d√©tect√©e)."

    )

# --- Incoh√©rences ---

with tab2:

    st.dataframe(incoherences, use_container_width=True, height=450)

    if incoherences.empty:

        st.info("Aucune incoh√©rence d√©tect√©e. üëç")

    else:

        st.caption(

            "Chaque ligne repr√©sente un groupe de valeurs qui devraient √™tre homog√®nes, "

            "mais qui sont √©crites diff√©remment. La colonne **Fichiers concern√©s** "

            "indique dans quels fichiers on trouve ces incoh√©rences."

        )

# --- Valeurs brutes ---

with tab3:

    st.dataframe(uniques, use_container_width=True, height=450)

    st.caption(

        "üßæ Ce tableau liste toutes les √©critures brutes (avant regroupement ou normalisation)."

    )

# -------------------- EXPORT EXCEL --------------------

st.write("")

st.write("")

st.subheader("üì§ Exporter les r√©sultats")

synthese_poles = (

    df["P√¥le"]

    .value_counts()

    .rename_axis("P√¥le")

    .reset_index(name="Nombre de t√¢ches")

    if "P√¥le" in df.columns

    else pd.DataFrame()

)

synthese_fichiers = (

    df["Source fichier"]

    .value_counts()

    .rename_axis("Source fichier")

    .reset_index(name="Nombre de lignes")

    if "Source fichier" in df.columns

    else pd.DataFrame()

)

dfs_export = {

    "Synthese_poles": synthese_poles,

    "Synthese_fichiers": synthese_fichiers,

    "Taux_remplissage": taux,

    "Valeurs_uniques": uniques,

    "Incoherences": incoherences,

    "Valeurs_propres": propres_final,

}

excel_bytes = to_excel_bytes(dfs_export)

st.write("")

st.download_button(

    label="üíæ T√©l√©charger l‚Äôanalyse compl√®te (Excel)",

    data=excel_bytes,

    file_name="analyse_taches_fusionnees.xlsx",

    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",

)

